{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vagdevijunnuri/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/vagdevijunnuri/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "import nltk \n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy as sp\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(trainingFile, testingFile):\n",
    "    \n",
    "    with open(trainingFile, \"r\") as fr1:\n",
    "        trainFile = fr1.readlines()\n",
    "    \n",
    "    with open(testingFile, \"r\") as fr2:\n",
    "        testFile = fr2.readlines()\n",
    "    \n",
    " \n",
    "    train_sentiments_t = [x.split(\"\\t\", 1)[0] for x in trainFile]\n",
    "    train_reviews_t = [x.split(\"\\t\", 1)[1] for x in trainFile]\n",
    "    \n",
    "    return train_reviews_t, testFile, train_sentiments_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews, test_reviews, train_sentiments = loadData('data/train.dat', 'data/test.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(reviews):\n",
    "      \n",
    "    clean_train_reviews = []\n",
    "   \n",
    "    for index, review in enumerate(reviews):\n",
    "        clean_train_reviews.append(preProcess(review))\n",
    "    \n",
    "    return clean_train_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(rawReview):\n",
    "\n",
    "    text_only = BeautifulSoup(rawReview).get_text()\n",
    "    \n",
    "    noEmail = re.sub(r'([\\w\\.-]+@[\\w\\.-]+\\.\\w+)','',text_only)\n",
    "    \n",
    "    noUrl = re.sub(r'(?i)\\b((?:[a-z][\\w-]+:(?:/{1,3}|[a-z0-9%])|www\\d{0,3}[.]| \\\n",
    "        [a-z0-9.\\-]+[.][a-z]{2,4}/|[a-z0-9.\\-]+[.][a-z])(?:[^\\s()<>]+|\\(([^\\s()<>]+| \\\n",
    "        (\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))','', noEmail)\n",
    "    \n",
    "    \n",
    "    smileys = \"\"\":-) :) :o) :D :-D :( :-( :o(\"\"\".split()\n",
    "    smileyPattern = \"|\".join(map(re.escape, smileys))\n",
    "    \n",
    "    letters_only = re.sub(\"[^a-zA-Z\" + smileyPattern + \"]\", \" \", noUrl)\n",
    "    \n",
    "    words = letters_only.lower().split()     \n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = ''\n",
    "    for word in words:\n",
    "        if word not in stops and len(word) > 3:\n",
    "        \n",
    "            lemmatized_words += str(lemmatizer.lemmatize(word)) + ' '\n",
    "    \n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTFIDFMatrices(train_data, test_data):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(norm = 'l2')\n",
    "    \n",
    "    train_matrix = vectorizer.fit_transform(train_data)\n",
    "    \n",
    "    test_matrix = vectorizer.transform(test_data)\n",
    "\n",
    "    return train_matrix, test_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csr_l2normalize(mat, copy=False, **kargs):\n",
    "    \n",
    "    if copy is True:\n",
    "        mat = mat.copy()\n",
    "    nrows = mat.shape[0]\n",
    "    nnz = mat.nnz\n",
    "    ind, val, ptr = mat.indices, mat.data, mat.indptr\n",
    "    \n",
    "    for i in range(nrows):\n",
    "        rsum = 0.0    \n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            rsum += val[j]**2\n",
    "        if rsum == 0.0:\n",
    "            continue\n",
    "        rsum = 1.0/np.sqrt(rsum)\n",
    "        for j in range(ptr[i], ptr[i+1]):\n",
    "            val[j] *= rsum\n",
    "            \n",
    "    if copy is True:\n",
    "        return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSimilarities(train_matrix, test_matrix):\n",
    "    \n",
    "    cosineSimilarities = np.dot(test_matrix, np.transpose(train_matrix))        \n",
    "    return cosineSimilarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = clean(train_reviews)\n",
    "test_reviews = clean(test_reviews)\n",
    "\n",
    "train_matrix, test_matrix = createTFIDFMatrices(train_reviews, test_reviews)\n",
    "train_matrix_norm = csr_l2normalize(train_matrix, copy=True)\n",
    "test_matrix_norm = csr_l2normalize(test_matrix, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = findSimilarities(train_matrix_norm, test_matrix_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find k neighbours\n",
    "import operator\n",
    "k = 501\n",
    "sims1 = similarities\n",
    "list_nbr = list()\n",
    "y_labels = list()\n",
    "for i in range(sims1.shape[0]):\n",
    "\n",
    "    count1 = 0\n",
    "    row = sims1.getrow(i).toarray()[0].ravel()\n",
    "    top_indices = row.argsort()[-k:]\n",
    "    top_values = row[row.argsort()[-k:]]\n",
    "    \n",
    "    for j in range(len(top_indices)):\n",
    "        if train_sentiments[top_indices[j]]=='+1':\n",
    "            count1 = count1+1\n",
    "    if count1>k/2:\n",
    "        y_labels.append('+1')\n",
    "    else:\n",
    "        y_labels.append('-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open('output1.dat', 'w')\n",
    "\n",
    "output.writelines( \"%s\\n\" % item for item in y_labels )\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
